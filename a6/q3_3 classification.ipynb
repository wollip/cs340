{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nHidden = [10]\n",
      "Training iteration = 2000, test error = 0.880000\n",
      "Training iteration = 4000, test error = 0.807000\n",
      "Training iteration = 6000, test error = 0.665000\n",
      "Training iteration = 8000, test error = 0.621000\n",
      "Training iteration = 10000, test error = 0.583000\n",
      "Training iteration = 12000, test error = 0.598000\n",
      "Training iteration = 14000, test error = 0.586000\n",
      "Training iteration = 16000, test error = 0.594000\n",
      "Training iteration = 18000, test error = 0.572000\n",
      "Training iteration = 20000, test error = 0.601000\n",
      "Training iteration = 22000, test error = 0.572000\n",
      "Training iteration = 24000, test error = 0.591000\n",
      "Training iteration = 26000, test error = 0.567000\n",
      "Training iteration = 28000, test error = 0.578000\n",
      "Training iteration = 30000, test error = 0.574000\n",
      "Training iteration = 32000, test error = 0.575000\n",
      "Training iteration = 34000, test error = 0.589000\n",
      "Training iteration = 36000, test error = 0.565000\n",
      "Training iteration = 38000, test error = 0.556000\n",
      "Training iteration = 40000, test error = 0.564000\n",
      "Training iteration = 42000, test error = 0.569000\n",
      "Training iteration = 44000, test error = 0.559000\n",
      "Training iteration = 46000, test error = 0.565000\n",
      "Training iteration = 48000, test error = 0.574000\n",
      "Training iteration = 50000, test error = 0.573000\n",
      "Training iteration = 52000, test error = 0.572000\n",
      "Training iteration = 54000, test error = 0.565000\n",
      "Training iteration = 56000, test error = 0.579000\n",
      "Training iteration = 58000, test error = 0.558000\n",
      "Training iteration = 60000, test error = 0.575000\n",
      "Training iteration = 62000, test error = 0.573000\n",
      "Training iteration = 64000, test error = 0.553000\n",
      "Training iteration = 66000, test error = 0.572000\n",
      "Training iteration = 68000, test error = 0.577000\n",
      "Training iteration = 70000, test error = 0.558000\n",
      "Training iteration = 72000, test error = 0.570000\n",
      "Training iteration = 74000, test error = 0.548000\n",
      "Training iteration = 76000, test error = 0.564000\n",
      "Training iteration = 78000, test error = 0.569000\n",
      "Training iteration = 80000, test error = 0.569000\n",
      "Training iteration = 82000, test error = 0.560000\n",
      "Training iteration = 84000, test error = 0.561000\n",
      "Training iteration = 86000, test error = 0.578000\n",
      "Training iteration = 88000, test error = 0.565000\n",
      "Training iteration = 90000, test error = 0.558000\n",
      "Training iteration = 92000, test error = 0.557000\n",
      "Training iteration = 94000, test error = 0.547000\n",
      "Training iteration = 96000, test error = 0.568000\n",
      "Training iteration = 98000, test error = 0.568000\n",
      "Training iteration = 100000, test error = 0.549000\n"
     ]
    }
   ],
   "source": [
    "include(\"example_usps.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf\n",
    "using JLD\n",
    "using PyPlot\n",
    "using Statistics\n",
    "include(\"misc.jl\")\n",
    "# Load X and y variable\n",
    "data = load(\"uspsData.jld\")\n",
    "(X,y,Xtest,ytest) = (data[\"X\"],data[\"y\"],data[\"Xtest\"],data[\"ytest\"])\n",
    "\n",
    "(n,d) = size(X)\n",
    "t = size(Xtest,1)\n",
    "\n",
    "# Standardize columns and add bias variable to input layer\n",
    "(X,mu,sigma) = standardizeCols(X)\n",
    "X = [ones(n,1) X]\n",
    "d += 1\n",
    "\n",
    "# Apply the same transformation to test data\n",
    "Xtest = standardizeCols(Xtest,mu=mu,sigma=sigma)\n",
    "Xtest = [ones(t,1) Xtest]\n",
    "\n",
    "# Let 'k' be the number of classes, and 'Y' be a matrix of binary labels\n",
    "k = maximum(y)\n",
    "Y = zeros(n,k)\n",
    "for i in 1:n\n",
    "\tY[i,y[i]] = 1\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train()\n",
    "    i = rand(1:n)\n",
    "    (f,g) = NeuralNetMulti_backprop(w,X[i,:],Y[i,:],k,nHidden)\n",
    "    g_total = g\n",
    "\tfor i in rand(1:n, batch_size - 1)\n",
    "        (f,g) = NeuralNetMulti_backprop(w,X[i,:],Y[i,:],k,nHidden)\n",
    "        g_total += g\n",
    "    end\n",
    "    global w = w - stepSize*(g_total/batch_size^0.5) - stepSize*L2*w + stepSize * momentum * (w - prev_w)\n",
    "    global prev_w = w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose network structure and randomly initialize weights\n",
    "include(\"NeuralNet.jl\")\n",
    "nHidden = [64,64,64,64]\n",
    "nParams = NeuralNetMulti_nParams(d,k,nHidden)\n",
    "w = randn(nParams,1)\n",
    "prev_w = w\n",
    "\n",
    "batch_size = 50\n",
    "# Train with stochastic gradient\n",
    "maxIter = 100000\n",
    "stepSize = 1e-3\n",
    "L2 = 1e-1\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 2000, test error = 0.650000\n",
      "Training iteration = 4000, test error = 0.515000\n",
      "Training iteration = 6000, test error = 0.507000\n",
      "Training iteration = 8000, test error = 0.475000\n",
      "Training iteration = 10000, test error = 0.431000\n",
      "Training iteration = 12000, test error = 0.364000\n",
      "Training iteration = 14000, test error = 0.300000\n",
      "Training iteration = 16000, test error = 0.233000\n",
      "Training iteration = 18000, test error = 0.187000\n",
      "Training iteration = 20000, test error = 0.153000\n",
      "Training iteration = 22000, test error = 0.136000\n",
      "Training iteration = 24000, test error = 0.122000\n",
      "Training iteration = 26000, test error = 0.109000\n",
      "Training iteration = 28000, test error = 0.091000\n",
      "Training iteration = 30000, test error = 0.087000\n",
      "Training iteration = 32000, test error = 0.083000\n",
      "Training iteration = 34000, test error = 0.081000\n",
      "Training iteration = 36000, test error = 0.076000\n",
      "Training iteration = 38000, test error = 0.078000\n",
      "Training iteration = 40000, test error = 0.070000\n",
      "Training iteration = 42000, test error = 0.071000\n",
      "Training iteration = 44000, test error = 0.070000\n",
      "Training iteration = 46000, test error = 0.071000\n",
      "Training iteration = 48000, test error = 0.075000\n",
      "Training iteration = 50000, test error = 0.072000\n",
      "Training iteration = 52000, test error = 0.071000\n",
      "Training iteration = 54000, test error = 0.071000\n",
      "Training iteration = 56000, test error = 0.070000\n",
      "Training iteration = 58000, test error = 0.072000\n",
      "Training iteration = 60000, test error = 0.068000\n",
      "Training iteration = 62000, test error = 0.067000\n",
      "Training iteration = 64000, test error = 0.068000\n",
      "Training iteration = 66000, test error = 0.068000\n",
      "Training iteration = 68000, test error = 0.070000\n",
      "Training iteration = 70000, test error = 0.067000\n",
      "Training iteration = 72000, test error = 0.066000\n",
      "Training iteration = 74000, test error = 0.070000\n",
      "Training iteration = 76000, test error = 0.065000\n",
      "Training iteration = 78000, test error = 0.068000\n",
      "Training iteration = 80000, test error = 0.069000\n",
      "Training iteration = 82000, test error = 0.069000\n",
      "Training iteration = 84000, test error = 0.067000\n",
      "Training iteration = 86000, test error = 0.068000\n",
      "Training iteration = 88000, test error = 0.066000\n",
      "Training iteration = 90000, test error = 0.070000\n",
      "Training iteration = 92000, test error = 0.067000\n",
      "Training iteration = 94000, test error = 0.070000\n",
      "Training iteration = 96000, test error = 0.070000\n",
      "Training iteration = 98000, test error = 0.068000\n",
      "Training iteration = 100000, test error = 0.069000\n"
     ]
    }
   ],
   "source": [
    "for iter in 1:maxIter\n",
    "\n",
    "    train()\n",
    "\n",
    "    # Every few iterations, plot the data/model:\n",
    "    if (mod(iter,round(maxIter/50)) == 0)\n",
    "        yhat = NeuralNet_predict(w,Xtest,k,nHidden)\n",
    "        @printf(\"Training iteration = %d, test error = %f\\n\",iter,sum(yhat .!= ytest)/t)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
