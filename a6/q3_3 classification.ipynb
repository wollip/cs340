{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nHidden = [10]\n",
      "Training iteration = 2000, test error = 0.864000\n",
      "Training iteration = 4000, test error = 0.712000\n",
      "Training iteration = 6000, test error = 0.632000\n",
      "Training iteration = 8000, test error = 0.586000\n",
      "Training iteration = 10000, test error = 0.604000\n",
      "Training iteration = 12000, test error = 0.603000\n",
      "Training iteration = 14000, test error = 0.595000\n",
      "Training iteration = 16000, test error = 0.575000\n",
      "Training iteration = 18000, test error = 0.583000\n",
      "Training iteration = 20000, test error = 0.587000\n",
      "Training iteration = 22000, test error = 0.594000\n",
      "Training iteration = 24000, test error = 0.595000\n",
      "Training iteration = 26000, test error = 0.601000\n",
      "Training iteration = 28000, test error = 0.591000\n",
      "Training iteration = 30000, test error = 0.606000\n",
      "Training iteration = 32000, test error = 0.600000\n",
      "Training iteration = 34000, test error = 0.581000\n",
      "Training iteration = 36000, test error = 0.558000\n",
      "Training iteration = 38000, test error = 0.578000\n",
      "Training iteration = 40000, test error = 0.580000\n",
      "Training iteration = 42000, test error = 0.584000\n",
      "Training iteration = 44000, test error = 0.589000\n",
      "Training iteration = 46000, test error = 0.584000\n",
      "Training iteration = 48000, test error = 0.583000\n",
      "Training iteration = 50000, test error = 0.588000\n",
      "Training iteration = 52000, test error = 0.587000\n",
      "Training iteration = 54000, test error = 0.574000\n",
      "Training iteration = 56000, test error = 0.561000\n",
      "Training iteration = 58000, test error = 0.562000\n",
      "Training iteration = 60000, test error = 0.559000\n",
      "Training iteration = 62000, test error = 0.569000\n",
      "Training iteration = 64000, test error = 0.578000\n",
      "Training iteration = 66000, test error = 0.568000\n",
      "Training iteration = 68000, test error = 0.578000\n",
      "Training iteration = 70000, test error = 0.565000\n",
      "Training iteration = 72000, test error = 0.575000\n",
      "Training iteration = 74000, test error = 0.555000\n",
      "Training iteration = 76000, test error = 0.572000\n",
      "Training iteration = 78000, test error = 0.571000\n",
      "Training iteration = 80000, test error = 0.575000\n",
      "Training iteration = 82000, test error = 0.586000\n",
      "Training iteration = 84000, test error = 0.576000\n",
      "Training iteration = 86000, test error = 0.576000\n",
      "Training iteration = 88000, test error = 0.579000\n",
      "Training iteration = 90000, test error = 0.571000\n",
      "Training iteration = 92000, test error = 0.559000\n",
      "Training iteration = 94000, test error = 0.563000\n",
      "Training iteration = 96000, test error = 0.574000\n",
      "Training iteration = 98000, test error = 0.554000\n",
      "Training iteration = 100000, test error = 0.562000\n"
     ]
    }
   ],
   "source": [
    "include(\"example_usps.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf\n",
    "using JLD\n",
    "using PyPlot\n",
    "using Statistics\n",
    "include(\"misc.jl\")\n",
    "# Load X and y variable\n",
    "data = load(\"uspsData.jld\")\n",
    "(X,y,Xtest,ytest) = (data[\"X\"],data[\"y\"],data[\"Xtest\"],data[\"ytest\"])\n",
    "\n",
    "(n,d) = size(X)\n",
    "t = size(Xtest,1)\n",
    "\n",
    "# Standardize columns and add bias variable to input layer\n",
    "(X,mu,sigma) = standardizeCols(X)\n",
    "X = [ones(n,1) X]\n",
    "d += 1\n",
    "\n",
    "# Apply the same transformation to test data\n",
    "Xtest = standardizeCols(Xtest,mu=mu,sigma=sigma)\n",
    "Xtest = [ones(t,1) Xtest]\n",
    "\n",
    "# Let 'k' be the number of classes, and 'Y' be a matrix of binary labels\n",
    "k = maximum(y)\n",
    "Y = zeros(n,k)\n",
    "for i in 1:n\n",
    "\tY[i,y[i]] = 1\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train()\n",
    "    i = rand(1:n)\n",
    "    (f,g) = NeuralNetMulti_backprop(w,X[i,:],Y[i,:],k,nHidden)\n",
    "    g_total = g\n",
    "\tfor i in rand(1:n, batch_size - 1)\n",
    "        (f,g) = NeuralNetMulti_backprop(w,X[i,:],Y[i,:],k,nHidden)\n",
    "        g_total += g\n",
    "    end\n",
    "    global prev_g = 0.2 * g_total/batch_size^0.5 + 0.8 *prev_g\n",
    "    \n",
    "    global w = w - stepSize*(prev_g) - stepSize*L2*w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose network structure and randomly initialize weights\n",
    "include(\"NeuralNet.jl\")\n",
    "nHidden = [16,16,16,16,16]\n",
    "nParams = NeuralNetMulti_nParams(d,k,nHidden)\n",
    "w = randn(nParams,1)\n",
    "prev_g = w * 0\n",
    "\n",
    "batch_size = 50\n",
    "# Train with stochastic gradient\n",
    "maxIter = 1000\n",
    "stepSize = 1e-4\n",
    "L2 = 1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration = 20, test error = 0.921000\n",
      "Training iteration = 40, test error = 0.920000\n",
      "Training iteration = 60, test error = 0.917000\n",
      "Training iteration = 80, test error = 0.919000\n",
      "Training iteration = 100, test error = 0.921000\n",
      "Training iteration = 120, test error = 0.921000\n",
      "Training iteration = 140, test error = 0.920000\n",
      "Training iteration = 160, test error = 0.918000\n",
      "Training iteration = 180, test error = 0.912000\n",
      "Training iteration = 200, test error = 0.910000\n",
      "Training iteration = 220, test error = 0.909000\n",
      "Training iteration = 240, test error = 0.913000\n",
      "Training iteration = 260, test error = 0.908000\n",
      "Training iteration = 280, test error = 0.909000\n",
      "Training iteration = 300, test error = 0.904000\n",
      "Training iteration = 320, test error = 0.908000\n",
      "Training iteration = 340, test error = 0.911000\n",
      "Training iteration = 360, test error = 0.907000\n",
      "Training iteration = 380, test error = 0.904000\n",
      "Training iteration = 400, test error = 0.910000\n",
      "Training iteration = 420, test error = 0.910000\n",
      "Training iteration = 440, test error = 0.911000\n",
      "Training iteration = 460, test error = 0.913000\n",
      "Training iteration = 480, test error = 0.911000\n",
      "Training iteration = 500, test error = 0.909000\n",
      "Training iteration = 520, test error = 0.907000\n",
      "Training iteration = 540, test error = 0.904000\n",
      "Training iteration = 560, test error = 0.906000\n",
      "Training iteration = 580, test error = 0.905000\n",
      "Training iteration = 600, test error = 0.900000\n",
      "Training iteration = 620, test error = 0.902000\n",
      "Training iteration = 640, test error = 0.903000\n",
      "Training iteration = 660, test error = 0.905000\n",
      "Training iteration = 680, test error = 0.905000\n",
      "Training iteration = 700, test error = 0.901000\n",
      "Training iteration = 720, test error = 0.906000\n",
      "Training iteration = 740, test error = 0.905000\n",
      "Training iteration = 760, test error = 0.908000\n",
      "Training iteration = 780, test error = 0.903000\n",
      "Training iteration = 800, test error = 0.903000\n",
      "Training iteration = 820, test error = 0.906000\n",
      "Training iteration = 840, test error = 0.900000\n",
      "Training iteration = 860, test error = 0.897000\n",
      "Training iteration = 880, test error = 0.903000\n",
      "Training iteration = 900, test error = 0.906000\n",
      "Training iteration = 920, test error = 0.907000\n",
      "Training iteration = 940, test error = 0.901000\n",
      "Training iteration = 960, test error = 0.903000\n",
      "Training iteration = 980, test error = 0.900000\n",
      "Training iteration = 1000, test error = 0.904000\n"
     ]
    }
   ],
   "source": [
    "for iter in 1:maxIter\n",
    "\n",
    "    train()\n",
    "\n",
    "\t# Every few iterations, plot the data/model:\n",
    "\tif (mod(iter,round(maxIter/50)) == 0)\n",
    "\t\tyhat = NeuralNet_predict(w,Xtest,k,nHidden)\n",
    "\t\t@printf(\"Training iteration = %d, test error = %f\\n\",iter,sum(yhat .!= ytest)/t)\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
